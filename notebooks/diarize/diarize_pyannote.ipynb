{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Speaker Diarization using pyannote-audio\n",
                "\n",
                "This notebook demonstrates how to perform speaker diarization using the `pyannote-audio` library. We will:\n",
                "1. Load and preprocess an audio file.\n",
                "2. Apply the pyannote-audio model to perform diarization.\n",
                "3. Output the diarization results in txt, json, and srt formats.\n",
                "\n",
                "## Explanation\n",
                "Speaker diarization is the process of partitioning an audio stream into homogeneous segments according to the speaker identity. This can be useful for analyzing conversations, meetings, and other multi-speaker audio recordings.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Requirements\n",
                "\n",
                "Install pyannote-audio and other necessary libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup installers\n",
                "commands = [\n",
                "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q pyannote.audio\", \"Install pyannote.audio\"),\n",
                "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q soundfile\", \"Install soundfile\"),\n",
                "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q numpy\", \"Install numpy\"),\n",
                "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q matplotlib\", \"Install matplotlib\"),\n",
                "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q transformers\", \"Install transformers\"),\n",
                "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q -U git+https://github.com/speechbrain/speechbrain.git@develop\", \"Install speechbrain\")\n",
                "]\n",
                "\n",
                "# Import the utils module which sets up the environment\n",
                "from modules import utils\n",
                "from modules import disable_warnings\n",
                "\n",
                "# Use LogTools\n",
                "log_tools = utils.LogTools()\n",
                "\n",
                "# Execute\n",
                "log_tools.command_state(commands)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Load Libraries and Discover GPU Resources"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import torch\n",
                "import numpy as np\n",
                "import soundfile as sf\n",
                "import matplotlib.pyplot as plt\n",
                "from pyannote.audio import Pipeline\n",
                "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
                "from scipy.signal import resample\n",
                "import IPython.display as ipd\n",
                "import os\n",
                "\n",
                "# Check to see what GPU resources are available\n",
                "def get_best_device():\n",
                "    if torch.cuda.is_available():\n",
                "        print(\"Using CUDA\")\n",
                "        return torch.device(\"cuda\")\n",
                "    elif torch.backends.mps.is_available():\n",
                "        print(\"Using MPS\")\n",
                "        return torch.device(\"mps\")\n",
                "    else:\n",
                "        print(\"Using CPU\")\n",
                "        return torch.device(\"cpu\")\n",
                "\n",
                "device = get_best_device()\n",
                "\n",
                "# Get HF_HUB_TOKEN from os environment\n",
                "HF_HUB_TOKEN = os.getenv(\"HF_HUB_TOKEN\")\n",
                "if HF_HUB_TOKEN:\n",
                "    print(\"HF_HUB_TOKEN found\")\n",
                "else:\n",
                "    print(\"HF_HUB_TOKEN not found!!!\")\n",
                "    exit(1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Load the Audio File\n",
                "\n",
                "We start by loading an audio file using `soundfile`. The audio needs to be in a format supported by `pyannote-audio`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the audio file\n",
                "audio_filepath = \"../../test_pcm.wav\"\n",
                "audio, sample_rate = sf.read(audio_filepath)\n",
                "\n",
                "# Test to figure out what format audio was loaded into \"audio\" as\n",
                "print(f\"Audio shape: {audio.shape}\")\n",
                "print(f\"Audio type: {type(audio)}\")\n",
                "print(f\"Audio dtype: {audio.dtype}\")\n",
                "print(f\"Audio sample rate: {sample_rate}\")\n",
                "\n",
                "# If audio sample_rate isn't 16000 then resample it with scipy.signal resample\n",
                "new_sample_rate = 16000\n",
                "if sample_rate != new_sample_rate:\n",
                "    # Resample audio\n",
                "    print(f\"Resampling audio file to {new_sample_rate}\")\n",
                "    num_samples = int(len(audio) * new_sample_rate / sample_rate)\n",
                "    audio = resample(audio, num_samples)\n",
                "    sample_rate = new_sample_rate\n",
                "    print(f\"New sample rate: {new_sample_rate}\")\n",
                "\n",
                "# Plot the audio waveform\n",
                "plt.figure(figsize=(15, 5))\n",
                "plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio)\n",
                "plt.title('Audio Waveform')\n",
                "plt.xlabel('Time [s]')\n",
                "plt.ylabel('Amplitude')\n",
                "plt.show()\n",
                "\n",
                "# Play the audio from memory\n",
                "import IPython.display as ipd\n",
                "ipd.Audio(audio, rate=sample_rate)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Apply pyannote-audio Diarization\n",
                "\n",
                "Next, we apply the pyannote-audio model to the audio file to perform diarization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the pyannote speaker diarization pipeline\n",
                "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=HF_HUB_TOKEN)\n",
                "\n",
                "# Set the device for the pipeline\n",
                "pipeline.to(device)\n",
                "\n",
                "# Convert numpy array to pyannote format\n",
                "waveform = {\"waveform\": torch.tensor(audio, dtype=torch.float32).unsqueeze(0).to(device), \"sample_rate\": sample_rate}\n",
                "\n",
                "# Apply the pipeline to the audio data\n",
                "with ProgressHook() as hook:\n",
                "  diarization = pipeline(waveform, hook=hook)\n",
                "\n",
                "# Print the diarization result\n",
                "print(diarization)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Free up Resources\n",
                "*Remove any local files and free up GPU resources.*\n",
                "\n",
                "Press the large red button below to get started! ðŸš€"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Remove the output files\n",
                "!rm -rf {txt_filepath} {json_filepath} {srt_filepath}\n",
                "print(\"Local files deleted\")\n",
                "\n",
                "# Free up GPU memory\n",
                "torch.cuda.empty_cache()\n",
                "print(\"GPU memory freed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "In this notebook, we demonstrated how to perform speaker diarization using the `pyannote-audio` library. We loaded and preprocessed the audio, applied the pyannote-audio model, and saved the diarization results in txt, json, and srt formats."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

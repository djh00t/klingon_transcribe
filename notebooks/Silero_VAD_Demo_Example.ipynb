{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
"# Voice Activity Detection (VAD) using Silero VAD\n\n",
"This notebook demonstrates how to perform Voice Activity Detection (VAD) using the `silero-vad` library. We will:\n",
"1. Load and preprocess an audio file.\n",
"2. Apply the VAD algorithm to detect speech segments.\n",
"3. Visualize and output the detected speech segments.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
            "## Step 1: Install and Import Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
"# this assumes that you have a relevant version of PyTorch installed\n",
"!pip install -q torchaudio\n",
"\n",
"SAMPLING_RATE = 16000\n",
"\n",
"import torch\n",
"torch.set_num_threads(1)\n",
"\n",
"from IPython.display import Audio\n",
"from pprint import pprint\n",
"# download example\n",
"torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')\n",
"USE_ONNX = False # change this to True if you want to test onnx model\n",
"if USE_ONNX:\n",
"    !pip install -q onnxruntime\n",
"\n",
"model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
"                              model='silero_vad',\n",
"                              force_reload=True,\n",
"                              onnx=USE_ONNX)\n",
"\n",
"(get_speech_timestamps,\n",
" save_audio,\n",
" read_audio,\n",
" VADIterator,\n",
" collect_chunks) = utils"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
    "## Step 2: Load the Audio File and Perform VAD
",
    "","We start by loading an audio file using `read_audio` from `silero-vad`. Then, we apply the VAD model to detect speech segments and save the speech-only audio."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
"# Load the audio file\n",
"wav = read_audio('en_example.wav', sampling_rate=SAMPLING_RATE)\n",
"\n",
"# get speech timestamps from full audio file\n",
"speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=SAMPLING_RATE)\n",
"pprint(speech_timestamps)\n",
"\n",
"# merge all speech chunks to one audio\n",
"save_audio('only_speech.wav',\n",
"           collect_chunks(speech_timestamps, wav), sampling_rate=SAMPLING_RATE)\n",
"Audio('only_speech.wav')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
    "## Step 3: Visualize the Detected Speech Segments
",
    "","We visualize the detected speech segments on the audio waveform to better understand where speech occurs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
"import matplotlib.pyplot as plt\n",
"import numpy as np\n",
"\n",
"# Plot the audio waveform\n",
"def plot_waveform(audio, sample_rate):\n",
"    plt.figure(figsize=(15, 5))\n",
"    plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio)\n",
"    plt.title('Audio Waveform')\n",
"    plt.xlabel('Time [s]')\n",
"    plt.ylabel('Amplitude')\n",
"    plt.show()\n",
"\n",
"plot_waveform(wav, SAMPLING_RATE)\n",
"\n",
"# Plot the audio waveform with detected speech segments\n",
"def plot_waveform_and_segments(audio, sample_rate, speech_segments):\n",
"    plt.figure(figsize=(15, 5))\n",
"    plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio, label='Audio')\n",
"    for segment in speech_segments:\n",
"        start, end = segment['start'] / sample_rate, segment['end'] / sample_rate\n",
"        plt.axvspan(start, end, color='red', alpha=0.5, label='Speech Segment' if start == speech_segments[0]['start'] / sample_rate else '')\n",
"    plt.title('Audio Waveform with Detected Speech Segments')\n",
"    plt.xlabel('Time [s]')\n",
"    plt.ylabel('Amplitude')\n",
"    plt.legend()\n",
"    plt.show()\n",
"\n",
"plot_waveform_and_segments(wav, SAMPLING_RATE, speech_timestamps)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
    "## Conclusion
",
    "","In this notebook, we demonstrated how to use the `silero-vad` library to detect speech segments in an audio file. We loaded and preprocessed the audio, applied the VAD algorithm, and visualized the detected speech segments. Optionally, we saved the detected speech segments as separate audio files for further analysis."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

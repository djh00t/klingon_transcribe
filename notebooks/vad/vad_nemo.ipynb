{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Activity Detection (VAD) using NeMo\n",
    "\n",
    "This notebook demonstrates how to perform Voice Activity Detection (VAD) using the `nemo` library. We will:\n",
    "1. Load and preprocess an audio file.\n",
    "2. Apply the VAD algorithm to detect speech segments.\n",
    "3. Visualize and output the detected speech segments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision numpy matplotlib soundfile nemo_toolkit[asr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Libraries and Discover GPU Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check to see what GPU resources are available\n",
    "def get_best_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA\")\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"Using MPS\")\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "        return \"cpu\"\n",
    "device = get_best_device()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the Audio File\n",
    "\n",
    "We start by loading an audio file using `soundfile`. The audio needs to be in a format supported by `nemo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "audio_filepath = \"../../test_pcm.wav\"\n",
    "audio, sample_rate = sf.read(audio_filepath)\n",
    "\n",
    "# Ensure the audio is a 1D array\n",
    "if audio.ndim > 1:\n",
    "    audio = np.mean(audio, axis=1)\n",
    "\n",
    "# Resample audio if necessary\n",
    "if sample_rate != 16000:\n",
    "    import resampy\n",
    "    audio = resampy.resample(audio, sample_rate, 16000)\n",
    "    sample_rate = 16000\n",
    "\n",
    "# Convert audio to torch tensor\n",
    "audio_tensor = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "# Plot the audio waveform\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio)\n",
    "plt.title('Audio Waveform')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply NeMo VAD\n",
    "\n",
    "Next, we initialize the `nemo` VAD model and apply it to the audio file to detect speech segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the NeMo VAD model\n",
    "vad_model = nemo_asr.models.EncDecClassificationModel.from_pretrained(model_name=\"vad_multilingual_marblenet\").to(device)\n",
    "\n",
    "# Move the audio tensor to the same device as the model\n",
    "audio_tensor = audio_tensor.to(device)\n",
    "\n",
    "# Perform VAD\n",
    "logits = vad_model(input_signal=audio_tensor, input_signal_length=torch.tensor([audio_tensor.shape[1]], dtype=torch.int64).to(device))\n",
    "probs = torch.softmax(logits, dim=-1).squeeze().cpu().detach().numpy()\n",
    "\n",
    "# Process VAD output\n",
    "vad_threshold = 0.5  # Threshold for detecting speech\n",
    "segments = []\n",
    "current_segment = None\n",
    "\n",
    "for i, prob in enumerate(probs):                                                                                                                                                                                                                                                      \n",
    "    if prob > vad_threshold:  # Class 1 corresponds to speech                                                                                                                                                                                                                         \n",
    "        if current_segment is None:                                                                                                                                                                                                                                                   \n",
    "            current_segment = [i * 0.02, (i + 1) * 0.02]                                                                                                                                                                                                                              \n",
    "    else:                                                                                                                                                                                                                                                                             \n",
    "        if current_segment is not None:                                                                                                                                                                                                                                               \n",
    "            current_segment[1] = (i + 1) * 0.02                                                                                                                                                                                                                                       \n",
    "            speech_segments.append(current_segment)                                                                                                                                                                                                                                   \n",
    "            current_segment = None\n",
    "\n",
    "if current_segment is not None:\n",
    "    segments.append(current_segment)\n",
    "\n",
    "# Print the VAD segments\n",
    "print(\"Detected speech segments (in seconds):\")\n",
    "for start, end in segments:\n",
    "    print(f\"Start: {start:.2f}, End: {end:.2f}\")\n",
    "\n",
    "# Print VAD statistics: Number of speech segments, total duration of speech\n",
    "# segments, and speech ratio\n",
    "num_speech_segments = len(segments)\n",
    "total_duration = sum([end - start for start, end in segments])\n",
    "speech_ratio = total_duration / (len(audio) / sample_rate)\n",
    "total_audio_length = len(audio) / sample_rate\n",
    "print(f\"\\nNumber of speech segments: {num_speech_segments}\")\n",
    "print(f\"Total length of audio: {total_audio_length:.2f} seconds\")\n",
    "print(f\"Total duration of speech segments: {total_duration:.2f} seconds\")\n",
    "print(f\"Speech ratio: {speech_ratio:.2f}\")\n",
    "print(f\"Segments: \\n{segments}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize the Detected Speech Segments\n",
    "\n",
    "We visualize the detected speech segments on the audio waveform to better understand where speech occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the audio waveform with detected speech segments\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio, label='Audio')\n",
    "for start, end in segments:\n",
    "    plt.axvspan(start, end, color='red', alpha=0.5, label='Speech Segment' if start == segments[0][0] else \"\")\n",
    "plt.title('Audio Waveform with Detected Speech Segments')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Models & Pipelines on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup models and pipelines from GPU memory\n",
    "# If device is cuda then cleanup cuda resources, if mps, cleanup mps resources\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "elif device == \"mps\":\n",
    "    torch.backends.mps.release_process_group()\n",
    "    torch.backends.mps.destroy_process_group()\n",
    "    torch.backends.mps.shutdown()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to use the `nemo` library to detect speech segments in an audio file. We loaded and preprocessed the audio, applied the VAD algorithm, and visualized the detected speech segments. Optionally, we saved the detected speech segments as separate audio files for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

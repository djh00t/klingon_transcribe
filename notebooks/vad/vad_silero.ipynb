{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Activity Detection (VAD) using Silero VAD\n",
    "\n",
    "This notebook demonstrates how to perform Voice Activity Detection (VAD) using the `silero-vad` library. We will:\n",
    "1. Load and preprocess an audio file.\n",
    "2. Apply the VAD algorithm to detect speech segments.\n",
    "3. Visualize and output the detected speech segments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup installers\n",
    "commands = [\n",
    "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q torch\", \"Install torch\"),\n",
    "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q torchvision\", \"Install torchvision\"),\n",
    "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q numpy\", \"Install numpy\"),\n",
    "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q matplotlib\", \"Install matplotlib\"),\n",
    "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q soundfile\", \"Install soundfile\"),\n",
    "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q silero-vad\", \"Install silero-vad\"),\n",
    "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q torchaudio\", \"Install torchaudio\")\n",
    "]\n",
    "\n",
    "# Import the utils module which sets up the environment\n",
    "from modules import utils\n",
    "from modules import disable_warnings\n",
    "\n",
    "# Use LogTools\n",
    "log_tools = utils.LogTools()\n",
    "\n",
    "# Execute\n",
    "log_tools.command_state(commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Libraries and Discover GPU Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='.*cudnn.*')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='.*RNN module weights.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see what GPU resources are available\n",
    "def get_best_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA\")\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"Using MPS\")\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "        return \"cpu\"\n",
    "device = get_best_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the Audio File\n",
    "\n",
    "We start by loading an audio file using `soundfile`. The audio needs to be in a format supported by `silero-vad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "audio_filepath = \"../../test_pcm.wav\"\n",
    "audio, sample_rate = sf.read(audio_filepath)\n",
    "\n",
    "# Ensure the audio is a 1D array\n",
    "if audio.ndim > 1:\n",
    "    audio = np.mean(audio, axis=1)\n",
    "\n",
    "# Resample audio if necessary\n",
    "if sample_rate != 16000:\n",
    "    import resampy\n",
    "    audio = resampy.resample(audio, sample_rate, 16000)\n",
    "    sample_rate = 16000\n",
    "\n",
    "# Convert audio to torch tensor\n",
    "audio_tensor = torch.tensor(audio, dtype=torch.float32)\n",
    "\n",
    "# Plot the audio waveform\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio)\n",
    "plt.title('Audio Waveform')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply Silero VAD\n",
    "\n",
    "Next, we initialize the `silero` VAD model and apply it to the audio file to detect speech segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Silero VAD model\n",
    "vad_model, utils = torch.hub.load(\n",
    "    repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=True, onnx=False\n",
    ")\n",
    "\n",
    "(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils\n",
    "\n",
    "# Apply the VAD model to the audio tensor\n",
    "speech_timestamps = get_speech_timestamps(\n",
    "    audio_tensor.numpy(), vad_model, sampling_rate=16000\n",
    ")\n",
    "\n",
    "# Extract speech segments\n",
    "speech_segments = [\n",
    "    (ts[\"start\"] / sample_rate, ts[\"end\"] / sample_rate) for ts in speech_timestamps\n",
    "]\n",
    "\n",
    "# Print the VAD segments\n",
    "print(\"Detected speech segments (in seconds):\")\n",
    "for start, end in speech_segments:\n",
    "    print(f\"Start: {start:.2f}, End: {end:.2f}\")\n",
    "\n",
    "# Print VAD statistics: Number of speech segments, total duration of speech\n",
    "# segments, and speech ratio\n",
    "num_speech_segments = len(speech_segments)\n",
    "total_duration = sum([end - start for start, end in speech_segments])\n",
    "speech_ratio = total_duration / (len(audio) / sample_rate)\n",
    "total_audio_length = len(audio) / sample_rate\n",
    "print(f\"\\nNumber of speech segments: {num_speech_segments}\")\n",
    "print(f\"Total length of audio: {total_audio_length:.2f} seconds\")\n",
    "print(f\"Total duration of speech segments: {total_duration:.2f} seconds\")\n",
    "print(f\"Speech ratio: {speech_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize the Detected Speech Segments\n",
    "\n",
    "We visualize the detected speech segments on the audio waveform to better understand where speech occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the audio waveform with detected speech segments\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio, label='Audio')\n",
    "for start, end in speech_segments:\n",
    "    plt.axvspan(start, end, color='red', alpha=0.5, label='Speech Segment' if start == speech_segments[0][0] else \"\")\n",
    "plt.title('Audio Waveform with Detected Speech Segments')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Stream Imitation Example\n",
    "\n",
    "This Stream Imitation Example demonstrates how to use the VADIterator class\n",
    "from the Silero VAD library to process audio in a streaming manner. This\n",
    "simulates real-time voice activity detection by processing small chunks of\n",
    "audio sequentially, rather than processing the entire audio file at once. This\n",
    "approach is particularly useful for applications where you need to perform VAD\n",
    "on live audio streams, such as during a phone call or a video conference.\n",
    "\n",
    "### Key Points\n",
    "**State Maintenance**\n",
    "\n",
    "The VADIterator class maintains the state of the VAD model across multiple\n",
    "chunks, allowing for continuous detection without losing context.\n",
    "\n",
    "**Real-Time Simulation**\n",
    "\n",
    "By processing audio in small chunks sequentially, this method simulates how you\n",
    "might perform VAD on a live audio stream.\n",
    "\n",
    "**Chunk-Based Processing**\n",
    "\n",
    "Each chunk is processed independently, but the state is preserved across\n",
    "chunks, which is crucial for accurate detection in streaming scenarios.\n",
    "\n",
    "**Example Output**\n",
    "\n",
    "When the speech_dict is printed, it might show something like:\n",
    "\n",
    "```python\n",
    "[{'start': 0.0, 'end': 0.1}] [{'start': 0.1, 'end': 0.2}]\n",
    "```\n",
    "\n",
    "This indicates that speech was detected in the first chunk from 0.0 to 0.1\n",
    "seconds, in the second chunk from 0.1 to 0.2 seconds, and so on.\n",
    "\n",
    "### Practical Applications\n",
    "**Live Streaming**\n",
    "\n",
    "Useful in applications such as live transcription services, voice-controlled\n",
    "systems, and real-time communication platforms where you need to detect voice\n",
    "activity on-the-fly.\n",
    "\n",
    "**Resource Efficiency**\n",
    "\n",
    "Processing audio in chunks can be more memory and CPU efficient, as it avoids\n",
    "loading and processing the entire audio file at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using VADIterator class\n",
    "# VADIterator is a helper class that maintains the state of the VAD model\n",
    "# across multiple audio chunks. This is important for streaming applications\n",
    "# where the model needs to remember the context between chunks.\n",
    "vad_iterator = VADIterator(vad_model.to(device))\n",
    "\n",
    "## Define the Window Size:\n",
    "# This variable defines the number of audio samples in each chunk. A typical\n",
    "# window size might be 1536 samples, which corresponds to approximately 96\n",
    "# milliseconds of audio at a 16 kHz sampling rate (1536 / 16000).\n",
    "window_size_samples = 1536\n",
    "\n",
    "## Loop Over the Audio in Chunks\n",
    "# Loop Initialization: The loop iterates over the audio file in steps of\n",
    "# window_size_samples. In each iteration, it extracts a chunk of audio.\n",
    "for i in range(0, len(audio), window_size_samples):\n",
    "    # Chunk Extraction: \n",
    "    # chunk = audio[i: i + window_size_samples] extracts a chunk of audio\n",
    "    # starting at index i and ending at i + window_size_samples.\n",
    "    chunk = audio[i : i + window_size_samples]\n",
    "    # Check for Incomplete Chunk: \n",
    "    # If the remaining audio is less than the window size, the loop breaks to\n",
    "    # avoid processing incomplete chunks.\n",
    "    if len(chunk) < window_size_samples:\n",
    "        break\n",
    "    # Process the Chunk: \n",
    "    # speech_dict = vad_iterator(chunk, return_seconds=True) processes the\n",
    "    # chunk using the vad_iterator. The return_seconds=True parameter indicates\n",
    "    # that the timestamps should be returned in seconds rather than sample\n",
    "    # indices.\n",
    "    chunk_tensor = torch.tensor(chunk, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    speech_dict = vad_iterator(chunk_tensor, return_seconds=True)\n",
    "    # Print Results: \n",
    "    # If the VAD model detects speech in the chunk, the results are printed.\n",
    "    # The results include timestamps of detected speech segments within the\n",
    "    # chunk.\n",
    "    if speech_dict:\n",
    "        print(speech_dict, end=' ')\n",
    "# Reset States:\n",
    "# After processing all chunks, vad_iterator.reset_states() resets the model's\n",
    "# internal states. This is necessary to clear the context when processing a new\n",
    "# audio stream or file.\n",
    "vad_iterator.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Get Just Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get speech probabilities for each chunk\n",
    "speech_probs = []\n",
    "for i in range(0, len(audio), window_size_samples):\n",
    "    chunk = audio[i: i + window_size_samples]\n",
    "    if len(chunk) < window_size_samples:\n",
    "        break\n",
    "    chunk_tensor = torch.tensor(chunk, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    speech_prob = vad_model(chunk_tensor, sample_rate).item()\n",
    "    speech_probs.append(speech_prob)\n",
    "vad_iterator.reset_states()  # reset model states after each audio\n",
    "\n",
    "print(speech_probs[:10])  # first 10 chunks predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Models & Pipelines on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup models and pipelines from GPU memory\n",
    "# If device is cuda then cleanup cuda resources, if mps, cleanup mps resources\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "elif device == \"mps\":\n",
    "    torch.backends.mps.release_process_group()\n",
    "    torch.backends.mps.destroy_process_group()\n",
    "    torch.backends.mps.shutdown()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to use the `silero-vad` library to detect speech segments in an audio file. We loaded and preprocessed the audio, applied the VAD algorithm, and visualized the detected speech segments. Optionally, we saved the detected speech segments as separate audio files for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

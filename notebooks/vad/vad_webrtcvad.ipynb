{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Activity Detection (VAD) using WebRTC VAD\n",
    "\n",
    "This notebook demonstrates how to perform Voice Activity Detection (VAD) using the `webrtcvad` library. We will:\n",
    "1. Load and preprocess an audio file.\n",
    "2. Apply the VAD algorithm to detect speech segments.\n",
    "3. Visualize and output the detected speech segments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup installers\n",
    "commands = [\n",
    "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q webrtcvad\", \"Install webrtcvad\"),\n",
    "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q numpy\", \"Install numpy\"),\n",
    "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q matplotlib\", \"Install matplotlib\"),\n",
    "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q soundfile\", \"Install soundfile\"),\n",
    "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q scipy\", \"Install scipy\")\n",
    "]\n",
    "\n",
    "# Import the utils module which sets up the environment\n",
    "from modules import utils\n",
    "from modules import disable_warnings\n",
    "\n",
    "# Use LogTools\n",
    "log_tools = utils.LogTools()\n",
    "\n",
    "# Execute\n",
    "log_tools.command_state(commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import webrtcvad\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Preprocess the Audio File\n",
    "\n",
    "We start by loading an audio file and converting it to a format suitable for processing by WebRTC VAD. The audio needs to be in 16-bit PCM format and have a valid sample rate (8000, 16000, 32000, or 48000 Hz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert audio to PCM format\n",
    "def convert_audio_to_pcm(audio, sample_rate):\n",
    "    # Ensure audio is mono\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "    \n",
    "    # Convert to 16-bit PCM\n",
    "    audio = (audio * 32767).astype(np.int16)\n",
    "    return audio\n",
    "\n",
    "# Load the audio file\n",
    "audio_filepath = \"../../test_pcm.wav\"\n",
    "audio, sample_rate = sf.read(audio_filepath)\n",
    "audio = convert_audio_to_pcm(audio, sample_rate)\n",
    "\n",
    "# Ensure sample rate is valid (must be 8000, 16000, 32000, or 48000 Hz)\n",
    "assert sample_rate in [8000, 16000, 32000, 48000], \"Invalid sample rate\"\n",
    "\n",
    "# Plot the audio waveform\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio)\n",
    "plt.title('Audio Waveform')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply WebRTC VAD\n",
    "\n",
    "Next, we apply the WebRTC VAD algorithm to detect speech segments in the audio.\n",
    "We split the audio into frames of 30 ms duration and check each frame for\n",
    "speech.\n",
    "\n",
    "Finally we print out the segments and audio file statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebRTC VAD\n",
    "vad = webrtcvad.Vad()\n",
    "vad.set_mode(3)  # 0: Normal, 3: Aggressive\n",
    "\n",
    "# Define the frame duration (must be 10, 20, or 30 ms)\n",
    "frame_duration = 30\n",
    "frame_length = int(sample_rate * frame_duration / 1000)\n",
    "assert len(audio) >= frame_length, \"Audio length is shorter than frame length\"\n",
    "\n",
    "# Split audio into frames\n",
    "frames = [audio[i:i + frame_length] for i in range(0, len(audio), frame_length) if len(audio[i:i + frame_length]) == frame_length]\n",
    "\n",
    "# Perform VAD\n",
    "speech_segments = []\n",
    "current_segment = None\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "    is_speech = vad.is_speech(frame.tobytes(), sample_rate)\n",
    "    if is_speech:\n",
    "        if current_segment is None:\n",
    "            current_segment = [i * frame_duration / 1000, (i + 1) * frame_duration / 1000]\n",
    "        else:\n",
    "            current_segment[1] = (i + 1) * frame_duration / 1000\n",
    "    else:\n",
    "        if current_segment is not None:\n",
    "            speech_segments.append(current_segment)\n",
    "            current_segment = None\n",
    "\n",
    "if current_segment is not None:\n",
    "    speech_segments.append(current_segment)\n",
    "\n",
    "# Print the VAD segments\n",
    "print(\"Detected speech segments (in seconds):\")\n",
    "for start, end in speech_segments:\n",
    "    print(f\"Start: {start:.2f}, End: {end:.2f}\")\n",
    "\n",
    "# Print VAD statistics: Number of speech segments, total duration of speech\n",
    "# segments, and speech ratio\n",
    "num_segments = len(speech_segments)\n",
    "total_duration = sum(end - start for start, end in speech_segments)\n",
    "speech_ratio = total_duration / (len(audio) / sample_rate)\n",
    "total_audio_length = len(audio) / sample_rate\n",
    "print(f\"\\nNumber of speech segments: {num_segments}\")\n",
    "print(f\"Total length of audio: {total_audio_length:.2f} seconds\")\n",
    "print(f\"Total duration of speech segments: {total_duration:.2f} seconds\")\n",
    "print(f\"Speech ratio: {speech_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize the Detected Speech Segments\n",
    "\n",
    "We visualize the detected speech segments on the audio waveform to better understand where speech occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the audio waveform with detected speech segments\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio, label='Audio')\n",
    "for start, end in speech_segments:\n",
    "    plt.axvspan(start, end, color='red', alpha=0.5, label='Speech Segment' if start == speech_segments[0][0] else \"\")\n",
    "plt.title('Audio Waveform with Detected Speech Segments')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to use the `webrtcvad` library to detect speech segments in an audio file. We loaded and preprocessed the audio, applied the VAD algorithm, and visualized the detected speech segments. Optionally, we saved the detected speech segments as separate audio files for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

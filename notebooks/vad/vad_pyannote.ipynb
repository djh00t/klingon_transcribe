{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Activity Detection (VAD) using Pyannote.audio\n",
    "\n",
    "This notebook demonstrates how to perform Voice Activity Detection (VAD) using the `pyannote.audio` library. We will:\n",
    "1. Load and preprocess an audio file.\n",
    "2. Apply the VAD algorithm to detect speech segments.\n",
    "3. Visualize and output the detected speech segments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision numpy matplotlib pyannote.audio soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Libraries and Discover GPU Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "from pyannote.core import Segment\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Check to see what GPU resources are available\n",
    "def get_best_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA\")\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"Using MPS\")\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "        return \"cpu\"\n",
    "device = get_best_device()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the Audio File\n",
    "\n",
    "We start by loading an audio file using `soundfile`. The audio needs to be in a format supported by `pyannote.audio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "audio_filepath = \"../test_pcm.wav\"\n",
    "audio, sample_rate = sf.read(audio_filepath)\n",
    "\n",
    "# Plot the audio waveform\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio)\n",
    "plt.title('Audio Waveform')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply Pyannote.audio VAD\n",
    "\n",
    "Next, we initialize the `pyannote.audio` VAD pipeline and apply it to the audio\n",
    "file to detect speech segments.\n",
    "\n",
    "Finally we print out the segments and audio file statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VAD pipeline\n",
    "pipeline = VoiceActivityDetection(segmentation=\"pyannote/segmentation\")\n",
    "\n",
    "# Apply the pipeline to the audio file\n",
    "vad_result = pipeline({\"uri\": \"test_audio\", \"audio\": audio_filepath})\n",
    "\n",
    "# Extract speech segments\n",
    "speech_segments = [(segment.start, segment.end) for segment in vad_result.get_timeline().support()]\n",
    "\n",
    "# Print the VAD segments\n",
    "print(\"Detected speech segments (in seconds):\")\n",
    "for start, end in speech_segments:\n",
    "    print(f\"Start: {start:.2f}, End: {end:.2f}\")\n",
    "\n",
    "# Print VAD statistics: Number of speech segments, total duration of speech\n",
    "# segments, and speech ratio\n",
    "num_speech_segments = len(speech_segments)\n",
    "total_duration = sum([end - start for start, end in speech_segments])\n",
    "speech_ratio = total_duration / len(audio)\n",
    "total_audio_length = len(audio) / sample_rate\n",
    "print(f\"\\nNumber of speech segments: {num_speech_segments}\")\n",
    "print(f\"Total length of audio: {total_audio_length:.2f} seconds\")\n",
    "print(f\"Total duration of speech segments: {total_duration:.2f} seconds\")\n",
    "print(f\"Speech ratio: {speech_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize the Detected Speech Segments\n",
    "\n",
    "We visualize the detected speech segments on the audio waveform to better understand where speech occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the audio waveform with detected speech segments\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio, label='Audio')\n",
    "for start, end in speech_segments:\n",
    "    plt.axvspan(start, end, color='red', alpha=0.5, label='Speech Segment' if start == speech_segments[0][0] else \"\")\n",
    "plt.title('Audio Waveform with Detected Speech Segments')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Models & Pipelines on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup models and pipelines from GPU memory\n",
    "# If device is cuda then cleanup cuda resources, if mps, cleanup mps resources\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "elif device == \"mps\":\n",
    "    torch.backends.mps.release_process_group()\n",
    "    torch.backends.mps.destroy_process_group()\n",
    "    torch.backends.mps.shutdown()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to use the `pyannote.audio` library to detect speech segments in an audio file. We loaded and preprocessed the audio, applied the VAD algorithm, and visualized the detected speech segments. Optionally, we saved the detected speech segments as separate audio files for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

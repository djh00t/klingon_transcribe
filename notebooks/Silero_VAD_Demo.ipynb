{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Voice Activity Detection (VAD) using Silero VAD\n",
                "\n",
                "This notebook demonstrates how to perform Voice Activity Detection (VAD) using the `silero-vad` library. We will:\n",
                "1. Load and preprocess an audio file.\n",
                "2. Apply the VAD algorithm to detect speech segments.\n",
                "3. Visualize and output the detected speech segments.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Requirements"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
                        "\u001b[0m"
                    ]
                }
            ],
            "source": [
                "# Install required packages\n",
                "!pip install -q torch torchvision numpy matplotlib silero-vad soundfile"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Load Libraries and Discover GPU Resources"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ImportError",
                    "evalue": "cannot import name 'get_speech_ts' from 'silero_vad' (/usr/local/envs/klingon_transcribe/lib/python3.11/site-packages/silero_vad/__init__.py)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msilero_vad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_speech_ts, VADIterator, collect_chunks\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msoundfile\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
                        "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_speech_ts' from 'silero_vad' (/usr/local/envs/klingon_transcribe/lib/python3.11/site-packages/silero_vad/__init__.py)"
                    ]
                }
            ],
            "source": [
                "# Import necessary libraries\n",
                "from silero_vad import get_speech_ts, VADIterator, collect_chunks\n",
                "import soundfile as sf\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "\n",
                "# Check to see what GPU resources are available\n",
                "def get_best_device():\n",
                "    if torch.cuda.is_available():\n",
                "        print(\"Using CUDA\")\n",
                "        return \"cuda\"\n",
                "    elif torch.backends.mps.is_available():\n",
                "        print(\"Using MPS\")\n",
                "        return \"mps\"\n",
                "    else:\n",
                "        print(\"Using CPU\")\n",
                "        return \"cpu\"\n",
                "device = get_best_device()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Load the Audio File\n",
                "\n",
                "We start by loading an audio file using `soundfile`. The audio needs to be in a format supported by `silero-vad`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the audio file\n",
                "audio_filepath = \"test.wav\"\n",
                "audio, sample_rate = sf.read(audio_filepath)\n",
                "\n",
                "# Plot the audio waveform\n",
                "plt.figure(figsize=(15, 5))\n",
                "plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio)\n",
                "plt.title('Audio Waveform')\n",
                "plt.xlabel('Time [s]')\n",
                "plt.ylabel('Amplitude')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Apply Silero VAD\n",
                "\n",
                "Next, we initialize the `silero-vad` model and apply it to the audio file to detect speech segments.\n",
                "\n",
                "Finally we print out the segments and audio file statistics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the VAD model\n",
                "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', device=device, force_reload=True)\n",
                "get_speech_ts = utils['get_speech_ts']\n",
                "\n",
                "# Apply the VAD model to the audio\n",
                "vad_segments = get_speech_ts(audio, model, sampling_rate=sample_rate)\n",
                "\n",
                "# Convert VAD segments to start and end times in seconds\n",
                "speech_segments = [(segment['start'] / sample_rate, segment['end'] / sample_rate) for segment in vad_segments]\n",
                "\n",
                "# Print the VAD segments\n",
                "print(\"Detected speech segments (in seconds):\")\n",
                "for start, end in speech_segments:\n",
                "    print(f\"Start: {start:.2f}, End: {end:.2f}\")\n",
                "\n",
                "# Print VAD statistics: Number of speech segments, total duration of speech\n",
                "# segments, and speech ratio\n",
                "num_speech_segments = len(speech_segments)\n",
                "total_duration = sum([end - start for start, end in speech_segments])\n",
                "speech_ratio = total_duration / (len(audio) / sample_rate)\n",
                "total_audio_length = len(audio) / sample_rate\n",
                "print(f\"\\nNumber of speech segments: {num_speech_segments}\")\n",
                "print(f\"Total length of audio: {total_audio_length:.2f} seconds\")\n",
                "print(f\"Total duration of speech segments: {total_duration:.2f} seconds\")\n",
                "print(f\"Speech ratio: {speech_ratio:.2f}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Visualize the Detected Speech Segments\n",
                "\n",
                "We visualize the detected speech segments on the audio waveform to better understand where speech occurs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the audio waveform with detected speech segments\n",
                "plt.figure(figsize=(15, 5))\n",
                "plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio, label='Audio')\n",
                "for start, end in speech_segments:\n",
                "    plt.axvspan(start, end, color='red', alpha=0.5, label='Speech Segment' if start == speech_segments[0][0] else \"\")\n",
                "plt.title('Audio Waveform with Detected Speech Segments')\n",
                "plt.xlabel('Time [s]')\n",
                "plt.ylabel('Amplitude')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cleanup Models & Pipelines on GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cleanup models and pipelines from GPU memory\n",
                "# If device is cuda then cleanup cuda resources, if mps, cleanup mps resources\n",
                "if device == \"cuda\":\n",
                "    torch.cuda.empty_cache()\n",
                "elif device == \"mps\":\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "In this notebook, we demonstrated how to use the `silero-vad` library to detect speech segments in an audio file. We loaded and preprocessed the audio, applied the VAD algorithm, and visualized the detected speech segments. Optionally, we saved the detected speech segments as separate audio files for further analysis."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

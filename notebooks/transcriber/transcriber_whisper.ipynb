{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Whisper Large Transcription\n",
                "\n",
                "This notebook demonstrates how to transcribe audio using the `whisper-large` model. We will:\n",
                "1. Load and preprocess an audio file.\n",
                "2. Apply the Whisper model to transcribe the audio.\n",
                "3. Output the transcription.\n",
                "\n",
                "## Explanation\n",
                "The Whisper model is a state-of-the-art speech recognition model that can transcribe audio into text. It is particularly useful for converting spoken language into written text, which can be used for various applications such as subtitles, transcription services, and more.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup installers\n",
                "commands = [\n",
                "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q openai-whisper\", \"Install whisper\"),\n",
                "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q torch\", \"Install torch\"),\n",
                "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q numpy\", \"Install numpy\"),\n",
                "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q soundfile\", \"Install soundfile\"),\n",
                "    (\"PIP_ROOT_USER_ACTION=ignore pip install -q tqdm\", \"Install tqdm\")\n",
                "]\n",
                "\n",
                "# Import the utils module which sets up the environment\n",
                "from modules import utils\n",
                "from modules import disable_warnings\n",
                "\n",
                "# Use LogTools\n",
                "log_tools = utils.LogTools()\n",
                "\n",
                "# Execute\n",
                "log_tools.command_state(commands)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Load Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import soundfile as sf\n",
                "import numpy as np\n",
                "import whisper\n",
                "import time\n",
                "import threading\n",
                "import torch\n",
                "from tqdm import tqdm\n",
                "import torch\n",
                "\n",
                "# Check to see what GPU resources are available\n",
                "def get_best_device():\n",
                "    if torch.cuda.is_available():\n",
                "        print(\"Using CUDA\")\n",
                "        return torch.device(\"cuda\")\n",
                "    elif torch.backends.mps.is_available():\n",
                "        print(\"Using MPS\")\n",
                "        return torch.device(\"mps\")\n",
                "    else:\n",
                "        print(\"Using CPU\")\n",
                "        return torch.device(\"cpu\")\n",
                "\n",
                "device = get_best_device()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Load the Audio File\n",
                "\n",
                "We start by loading an audio file using `soundfile`. The audio needs to be in a\n",
                "format supported by `whisper`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the audio file\n",
                "audio_filepath = \"../../test_pcm.wav\"\n",
                "audio, sample_rate = sf.read(audio_filepath)\n",
                "\n",
                "# Display the audio properties\n",
                "print(f\"Audio sample rate: {sample_rate} Hz\")\n",
                "print(f\"Audio duration: {len(audio) / sample_rate:.2f} seconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Transcribe the Audio\n",
                "\n",
                "Next, we apply the Whisper model to transcribe the audio file.\n",
                "\n",
                "### Model Options\n",
                "The Whisper model comes in several sizes, each with different performance characteristics:\n",
                "- `tiny`\n",
                "- `tiny.en`\n",
                "- `base`\n",
                "- `base.en`\n",
                "- `small`\n",
                "- `small.en`\n",
                "- `medium`\n",
                "- `medium.en`\n",
                "- `large`\n",
                "- `large-v2`\n",
                "- `large-v3`\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set Whisper variables\n",
                "model_name = \"large-v3\"\n",
                "batch_size = 16 # reduce if low on GPU mem\n",
                "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
                "\n",
                "# Function to update transcription progress\n",
                "def update_progress_bar(progress_bar, duration, interval=1):\n",
                "    for i in range(0, duration, interval):\n",
                "        time.sleep(interval)\n",
                "        progress_bar.update(interval)\n",
                "    progress_bar.update(duration % interval)\n",
                "\n",
                "print(\"Starting transcription process...\")\n",
                "\n",
                "# Function to display model loading progress\n",
                "def load_model_with_progress(model_name):\n",
                "    print(\"Loading Whisper model...\")\n",
                "    progress_bar = tqdm(total=100, desc=\"Model Loading\", bar_format=\"{l_bar}{bar:50}| {n_fmt}/{total_fmt} {elapsed}\")\n",
                "\n",
                "    model = whisper.load_model(model_name)\n",
                "    for i in range(100):\n",
                "        time.sleep(0.1)\n",
                "        progress_bar.update(1)\n",
                "    progress_bar.close()\n",
                "    return model\n",
                "\n",
                "# Load the Whisper model with progress\n",
                "model = load_model_with_progress(model_name)\n",
                "\n",
                "# Create a progress bar for transcription\n",
                "duration = int(len(audio) / sample_rate)\n",
                "progress_bar = tqdm(total=duration, desc=\"Transcribing audio\", bar_format=\"{l_bar}{bar:50}| {n_fmt}/{total_fmt} {elapsed}\")\n",
                "\n",
                "# Start the progress update in a separate thread\n",
                "progress_thread = threading.Thread(target=update_progress_bar, args=(progress_bar, duration))\n",
                "progress_thread.start()\n",
                "\n",
                "# Transcribe the audio\n",
                "result = model.transcribe(audio, compute_type=\"float16\", batch_size=32, verbose=True)\n",
                "# result = model.transcribe(audio)\n",
                "\n",
                "# Wait for the progress thread to finish and close the progress bar\n",
                "progress_thread.join()\n",
                "progress_bar.close()\n",
                "\n",
                "\n",
                "# Output the transcription\n",
                "transcription = result['text']\n",
                "print(\"\\nTranscript:\")\n",
                "print(transcription)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Free up Resources\n",
                "*Remove any local files and free up GPU resources.*\n",
                "\n",
                "Press the large red button below to get started! ðŸš€"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Delete the model object\n",
                "del model\n",
                "\n",
                "# Free up GPU memory\n",
                "torch.cuda.empty_cache()\n",
                "print(\"GPU memory freed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "In this notebook, we demonstrated how to transcribe audio using `whisper` models. We loaded and preprocessed the audio, applied the Whisper model, and output the transcription."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
